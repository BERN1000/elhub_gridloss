{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d046b898",
   "metadata": {},
   "source": [
    "# Elhub API data - Gridloss - Summerproject 2025\n",
    "\n",
    "## Missing values\n",
    "\n",
    "Bjørn Eirik Rognskog Nordbak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e9383b",
   "metadata": {},
   "source": [
    "### Importing data from Elhub API\n",
    "https://api.elhub.no/energy-data-api#/grid-areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12afb95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "oslo = ZoneInfo(\"Europe/Oslo\")\n",
    "\n",
    "def fetch_window(start_dt, end_dt):\n",
    "    params = {\n",
    "        \"dataset\":   \"LOSS_PER_MGA_HOUR\",\n",
    "        \"startDate\": start_dt.isoformat(),\n",
    "        \"endDate\":   end_dt.isoformat(),\n",
    "    }\n",
    "    url = \"https://api.elhub.no/energy-data/v0/grid-areas\"\n",
    "    resp = requests.get(url, params=params)\n",
    "    obj = resp.json()\n",
    "    \n",
    "    # --- safeguard: if there's no \"data\", bail with empty DF ----\n",
    "    raw = obj.get(\"data\")\n",
    "    if raw is None:\n",
    "        print(f\"  → no 'data' for {start_dt.date()} → {end_dt.date()}, skipping\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # otherwise flatten\n",
    "    df = pd.json_normalize(\n",
    "        raw,\n",
    "        record_path=[\"attributes\", \"lossPerMgaHour\"],\n",
    "        meta=[\n",
    "            [\"attributes\", \"eic\"],\n",
    "            [\"attributes\", \"name\"],\n",
    "            [\"attributes\", \"status\"],\n",
    "        ],\n",
    "        errors=\"ignore\"\n",
    "    ).rename(columns={\n",
    "        \"attributes.eic\":    \"eic\",\n",
    "        \"attributes.name\":   \"name\",\n",
    "        \"attributes.status\": \"status\",\n",
    "    })\n",
    "    return df\n",
    "\n",
    "# loop as before\n",
    "span_start = datetime(2023,1,1,0,0, tzinfo=oslo)\n",
    "span_end   = datetime(2025,6,1,0,0, tzinfo=oslo)\n",
    "window = timedelta(days=7)\n",
    "\n",
    "all_chunks = []\n",
    "cur = span_start\n",
    "while cur < span_end:\n",
    "    nxt = min(cur + window, span_end)\n",
    "    print(f\"Fetching {cur.date()} → {nxt.date()}\")\n",
    "    dfc = fetch_window(cur, nxt)\n",
    "    all_chunks.append(dfc)\n",
    "    cur = nxt\n",
    "\n",
    "big_df = pd.concat(all_chunks, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa81b5d2",
   "metadata": {},
   "source": [
    "### Reading data from premade CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947115b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "big_df = pd.read_csv('big_df.csv')\n",
    "\n",
    "big_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0368ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Summary with .info()\n",
    "#    This shows you how many non-null rows each column has.\n",
    "big_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb7d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Count of missing per column\n",
    "missing_counts = big_df.isnull().sum()\n",
    "print(missing_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c6f399",
   "metadata": {},
   "source": [
    "### Check for temporal gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a48244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Parse your timestamps & sort\n",
    "df = big_df.copy()\n",
    "df['startTime'] = pd.to_datetime(df['startTime'], utc=True)\n",
    "df = df.sort_values(['eic', 'startTime'])\n",
    "\n",
    "# 2) Compute the difference between each timestamp and the previous one\n",
    "df['delta'] = df.groupby('eic')['startTime'].diff()\n",
    "\n",
    "# 3) Find all cases where that delta isn’t exactly 1 hour\n",
    "gaps = df[(df['delta'].notna()) & (df['delta'] != pd.Timedelta(hours=1))].copy()\n",
    "\n",
    "# 4) For clarity, pull in the “previous timestamp” and the size of the gap\n",
    "gaps['prevTime']  = gaps.groupby('eic')['startTime'].shift(1)\n",
    "gaps['gapHours'] = gaps['delta'].dt.total_seconds() / 3600\n",
    "\n",
    "# 5) Show the gaps\n",
    "gaps[['eic', 'prevTime', 'startTime', 'gapHours']].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5690a68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Parse your timestamps & sort\n",
    "df = big_df.copy()\n",
    "df['startTime'] = pd.to_datetime(df['startTime'], utc=True)\n",
    "df = df.sort_values(['name', 'startTime'])\n",
    "\n",
    "# 2) Compute the difference between each timestamp and the previous one, per name\n",
    "df['delta_name'] = df.groupby('name')['startTime'].diff()\n",
    "\n",
    "# 3) Find all cases where that delta isn’t exactly 1 hour\n",
    "gaps_name = df[(df['delta_name'].notna()) & (df['delta_name'] != pd.Timedelta(hours=1))].copy()\n",
    "\n",
    "# 4) For clarity, pull in the “previous timestamp” and the size of the gap\n",
    "gaps_name['prevTime']  = gaps_name.groupby('name')['startTime'].shift(1)\n",
    "gaps_name['gapHours'] = gaps_name['delta_name'].dt.total_seconds() / 3600\n",
    "\n",
    "# 5) Show the gaps\n",
    "gaps_name[['name', 'prevTime', 'startTime', 'gapHours']].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1881d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 140)  # Show only 140 rows, scroll for more\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you already have 'gaps_name' from your existing gap detection logic\n",
    "# Ensure gap start and end columns are present\n",
    "gaps_name['gap_start'] = gaps_name['prevTime']\n",
    "gaps_name['gap_end'] = gaps_name['startTime']\n",
    "\n",
    "# Group by 'name' and compute summary statistics\n",
    "gap_overview = (\n",
    "    gaps_name\n",
    "    .groupby('name')\n",
    "    .agg(\n",
    "        n_gaps=('gapHours', 'count'),\n",
    "        total_gap_hours=('gapHours', 'sum'),\n",
    "        first_gap_start=('gap_start', 'min'),\n",
    "        last_gap_end=('gap_end', 'max')\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(by='n_gaps', ascending=False)\n",
    ")\n",
    "\n",
    "# Optional: round or cast total gap hours\n",
    "gap_overview['total_gap_hours'] = gap_overview['total_gap_hours'].round(1)\n",
    "\n",
    "# Display\n",
    "print(\"\\n--- Missing Windows Overview ---\")\n",
    "gap_overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bf5ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 280)  # Show only 140 rows, scroll for more\n",
    "# Ensure these columns are in place\n",
    "gaps_name['gap_start'] = gaps_name['prevTime']\n",
    "gaps_name['gap_end'] = gaps_name['startTime']\n",
    "\n",
    "# Select and rename for clarity\n",
    "missing_windows = (\n",
    "    gaps_name[['name', 'gap_start', 'gap_end', 'gapHours']]\n",
    "    .sort_values(by=['name', 'gap_start'])\n",
    "    .reset_index(drop=True)\n",
    "    .rename(columns={'gapHours': 'gap_duration_hours'})\n",
    ")\n",
    "\n",
    "# Show result\n",
    "print(\"\\n--- All Missing Time Windows ---\")\n",
    "display(missing_windows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8403eb7b",
   "metadata": {},
   "source": [
    "### Statisics on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea995169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Basic descriptive stats for all numeric columns\n",
    "stats = big_df.describe()\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ec89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Boxplots to spot outliers\n",
    "big_df.select_dtypes('number').plot.box(figsize=(8, 6))\n",
    "plt.title('Boxplots of Numeric Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e24f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: pandas ≥1.5 has numeric_only parameter\n",
    "corr = big_df.corr(numeric_only=True)\n",
    "\n",
    "# Option B: select numeric dtypes yourself\n",
    "import numpy as np\n",
    "num_df = big_df.select_dtypes(include=[np.number])\n",
    "corr = num_df.corr()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # optional, but nicer\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", square=True)\n",
    "plt.title('Correlation Matrix (numeric only)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be99ab29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gridloss_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
